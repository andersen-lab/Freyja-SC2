name: Process samples

on:
  schedule:
    - cron: '0 */2 * * *'
env:
  BATCH_SIZE: 100
  

jobs:
  run_samples:
    permissions:
      contents: write
      id-token: write

    runs-on: self-hosted

    steps:
      - name: Checkout main
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Setup nextflow
        uses: nf-core/setup-nextflow@v1
      
      - name: Setup Python
        run: |
              echo ${{secrets.DPILZ_USR_PWD}} | sudo -S dnf install python3 -y
              echo ${{secrets.DPILZ_USR_PWD}} | sudo -S dnf install python3-pip -y
              pip3 install pandas numpy pyyaml ffq epiweeks

      - name: Setup AWS CLI
        run : |
              curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
              unzip awscliv2.zip
              echo ${{secrets.DPILZ_USR_PWD}} | sudo -S ./aws/install --update
              
      - name: 'Set up Cloud SDK'
        uses: 'google-github-actions/setup-gcloud@v2'
        with:
          version: '>= 363.0.0'
      
      - id: 'auth'
        name: 'Authenticate with gcloud'
        uses: 'google-github-actions/auth@v2'
        with:
          workload_identity_provider: 'projects/12767718289/locations/global/workloadIdentityPools/github/providers/freyja-sra'
          service_account: 'outbreak-ww@andersen-lab-primary.iam.gserviceaccount.com'

      - name: 'Get accession list'
        run: |
              python scripts/get_accession_list_wendy.py $BATCH_SIZE

      - name: Run pipeline on new samples
        run: |
                export NXF_ENABLE_VIRTUAL_THREADS=false
                nextflow run main.nf \
                  --accession_list data/accession_list.csv \
                  --num_samples $BATCH_SIZE \
                  -profile docker \
                  -entry sra &
                BG_PID=$!
                wait $BG_PID
          
      - name: Aggregate outputs
        run: |
              python scripts/aggregate_demix.py
              python scripts/aggregate_variants.py
              python scripts/aggregate_metadata.py

      - id: 'download-aggregated-outputs'
        name: 'Download aggregated outputs'
        run: |
              gcloud storage cp gs://outbreak-ww-data/aggregate/aggregate_demix.json outputs/aggregate/aggregate_demix.json --billing-project=andersen-lab-primary
              gcloud storage cp gs://outbreak-ww-data/aggregate/aggregate_variants.json outputs/aggregate/aggregate_variants.json --billing-project=andersen-lab-primary
              gcloud storage cp gs://outbreak-ww-data/aggregate/aggregate_metadata.json outputs/aggregate/aggregate_metadata.json  --billing-project=andersen-lab-primary

      - id: 'concatenate-outputs'
        name: 'Concatenate outputs'
        run: |
              python scripts/concat_agg_files.py
      
      - id: 'create-demix-by-week'
        name: 'Create demix by week'
        run: |
              python scripts/aggregate_demix_by_week.py

      - id: 'upload-outputs'
        name: 'Upload Outputs to Cloud Storage'
        uses: 'google-github-actions/upload-cloud-storage@v2'
        with:
          path: 'outputs/'
          destination: 'outbreak-ww-data/'
          parent: false
          project_id: 'andersen-lab-primary'

      - name: 'Update processed samples'
        run: |
              python scripts/update_sample_status.py $BATCH_SIZE
      
      - name: 'Commit and push changes'
        run: |
              git config --local user.name "$GITHUB_ACTOR"
              git config --local user.email "$GITHUB_ACTOR@users.noreply.github.com"
              git remote set-url origin https://github.com/andersen-lab/Freyja-SRA
              git add data/all_metadata.csv
              git commit -m "Update processed samples"
              git push --force

      - name: 'Clean workspace'
        run: |
              rm -rf ${{ github.workspace }}/*